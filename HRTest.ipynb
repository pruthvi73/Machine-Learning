{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def create_mask_from_annotation(annot, img_size):\n",
    "    mask = Image.new('L', img_size, 0)\n",
    "    for shape in annot['shapes']:\n",
    "        polygon = [(float(x), float(y)) for x, y in shape['points']]\n",
    "        ImageDraw.Draw(mask).polygon(polygon, outline=1, fill=1)\n",
    "    return np.array(mask)\n",
    "\n",
    "def load_image_and_mask(img_path, annot_path, img_size=(128, 128)):\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize(img_size)\n",
    "    img = np.array(img, dtype=np.float32) / 255.0  # Normalize and cast to float32\n",
    "\n",
    "    with open(annot_path, 'r') as file:\n",
    "        annot = json.load(file)\n",
    "    mask = create_mask_from_annotation(annot, img_size)\n",
    "    mask = np.array(mask, dtype=np.uint8)  # Cast mask to uint8\n",
    "    mask = mask.reshape((*mask.shape, 1))  # Add channel dimension\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(image_dir, annotation_dir, batch_size=16):\n",
    "    image_paths = sorted([str(path) for path in Path(image_dir).glob(\"*.jpg\")])\n",
    "    annotation_paths = sorted([str(path) for path in Path(annotation_dir).glob(\"*.json\")])\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, annotation_paths))\n",
    "    dataset = dataset.map(lambda x, y: tf.numpy_function(\n",
    "                            load_image_and_mask, \n",
    "                            [x, y], \n",
    "                            [tf.float32, tf.uint8]),  # Specify output types here\n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x, y: (tf.reshape(x, [128, 128, 3]), tf.reshape(y, [128, 128, 1])))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "train_dataset = load_dataset('path/to/data/image', 'path/to/data/annotations')\n",
    "val_dataset = load_dataset('path/to/data/image', 'path/to/data/annotations')\n",
    "test_dataset = load_dataset('path/to/data/image', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING AND RUNNING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HRnet as HRNet\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "HRNet.model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])  # Replace 'accuracy' with your IoU metrics\n",
    "\n",
    "# Training parameters\n",
    "total_epochs = 90\n",
    "report_interval = 30\n",
    "\n",
    "# Lists to store metrics\n",
    "metrics_history = []\n",
    "\n",
    "# Training and Evaluation Loop\n",
    "for epoch in range(total_epochs):\n",
    "    print(f\"Starting epoch {epoch + 1}/{total_epochs}\")\n",
    "\n",
    "    # Training step\n",
    "    for x_batch_train, y_batch_train in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = HRNet.model(x_batch_train, training=True)\n",
    "            loss_value = tf.keras.losses.sparse_categorical_crossentropy(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, HRNet.model.trainable_weights)\n",
    "        HRNet.model.optimizer.apply_gradients(zip(grads, HRNet.model.trainable_weights))\n",
    "\n",
    "    # Validation and Test step at specified intervals\n",
    "    if (epoch + 1) % report_interval == 0 or epoch == total_epochs - 1:\n",
    "        val_miou, test_miou = [], []\n",
    "        # Validation Evaluation\n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_iou_list = HRNet.IoU_per_class(y_batch_val, HRNet.model(x_batch_val, training=False)) # Replace with actual IoU calculation\n",
    "            val_miou_value = HRNet.compute_mIoU(y_batch_val, HRNet.model(x_batch_val, training=False)) # Replace with actual mIoU calculation\n",
    "            val_miou.append(val_miou_value)\n",
    "\n",
    "        # Test Evaluation\n",
    "        for x_batch_test, y_batch_test in test_dataset:\n",
    "            test_iou_list = HRNet.IoU_per_class(y_batch_test, HRNet.model(x_batch_test, training=False)) # Replace with actual IoU calculation\n",
    "            test_miou_value = HRNet.compute_mIoU(y_batch_test, HRNet.model(x_batch_test, training=False)) # Replace with actual mIoU calculation\n",
    "            test_miou.append(test_miou_value)\n",
    "\n",
    "        avg_val_miou = np.mean(val_miou)\n",
    "        avg_test_miou = np.mean(test_miou)\n",
    "\n",
    "        metrics_history.append({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Validation mIoU': avg_val_miou,\n",
    "            'Test mIoU': avg_test_miou\n",
    "        })\n",
    "        print(f'Epoch {epoch + 1} - Validation mIoU: {avg_val_miou}, Test mIoU: {avg_test_miou}')\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(metrics_history)\n",
    "results_df.to_csv('training_results.csv', index=False)\n",
    "\n",
    "# Visualization\n",
    "epochs = results_df['Epoch']\n",
    "val_miou = results_df['Validation mIoU']\n",
    "test_miou = results_df['Test mIoU']\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(epochs, val_miou, label='Validation mIoU', color='blue')\n",
    "plt.plot(epochs, test_miou, label='Test mIoU', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean IoU')\n",
    "plt.title('Mean IoU over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

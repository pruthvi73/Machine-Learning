{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYtfga-o0jgr",
        "outputId": "2eaa94c1-4e8e-4f57-8acd-de45ba147711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "45v0pC5X1nJI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [64, 64])  # Resize images to match HRNet input\n",
        "    img /= 127.0  # Normalize to [0,1]\n",
        "    return img\n",
        "\n",
        "def load_dataset(image_dir):\n",
        "    image_paths = [str(path) for path in Path(image_dir).glob(\"*.jpg\")]\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    dataset = dataset.map(preprocess_image)\n",
        "    return dataset\n",
        "\n",
        "# Load your dataset\n",
        "train_dataset = load_dataset('/content/drive/MyDrive/dacl10k_v2_devphase/dacl10k_v2_devphase/images/train')\n",
        "train_dataset = train_dataset.batch(32)  # Set the batch size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sPjskoBo0w7A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image, ImageDraw\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "def create_mask_from_annotation(annot, img_size):\n",
        "    mask = Image.new('L', img_size, 0)\n",
        "    for shape in annot['shapes']:\n",
        "        polygon = [(float(x), float(y)) for x, y in shape['points']]\n",
        "        ImageDraw.Draw(mask).polygon(polygon, outline=1, fill=1)\n",
        "    return np.array(mask)\n",
        "\n",
        "def load_image_and_mask(img_path, annot_path, img_size=(64, 64)):\n",
        "    img = Image.open(img_path)\n",
        "    img = img.resize(img_size)\n",
        "    img = np.array(img, dtype=np.float32) / 255.0  # Normalize and cast to float32\n",
        "\n",
        "    with open(annot_path, 'r') as file:\n",
        "        annot = json.load(file)\n",
        "    mask = create_mask_from_annotation(annot, img_size)\n",
        "    mask = np.array(mask, dtype=np.uint8)  # Cast mask to uint8\n",
        "    mask = mask.reshape((*mask.shape, 1))  # Add channel dimension\n",
        "\n",
        "    return img, mask\n",
        "\n",
        "def calculate_iou(y_true, y_pred, smooth=1e-6):\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
        "    iou = (intersection + smooth) / (union + smooth)\n",
        "    return iou\n",
        "\n",
        "def calculate_miou(y_true, y_pred, num_classes):\n",
        "    iou_list = []\n",
        "    for i in range(num_classes):\n",
        "        y_true_i = tf.cast(y_true == i, tf.float32)\n",
        "        y_pred_i = tf.cast(y_pred == i, tf.float32)\n",
        "        iou_list.append(calculate_iou(y_true_i, y_pred_i))\n",
        "    miou = tf.reduce_mean(iou_list)\n",
        "    return miou\n",
        "\n",
        "def segmentation_accuracy(y_true, y_pred):\n",
        "    # Convert y_pred to label indices and reshape to match y_true shape\n",
        "    y_pred_labels = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = tf.expand_dims(y_pred_labels, axis=-1)\n",
        "\n",
        "    # Ensure y_true and y_pred_labels have the same shape\n",
        "    y_true = tf.cast(y_true, tf.int64)\n",
        "    y_pred_labels = tf.cast(y_pred_labels, tf.int64)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct_prediction = tf.equal(y_pred_labels, y_true)\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LV2bl-QO1B8Z"
      },
      "outputs": [],
      "source": [
        "def load_dataset(image_dir, annotation_dir, batch_size=16):\n",
        "    image_paths = sorted([str(path) for path in Path(image_dir).glob(\"*.jpg\")])\n",
        "    annotation_paths = sorted([str(path) for path in Path(annotation_dir).glob(\"*.json\")])\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, annotation_paths))\n",
        "    dataset = dataset.map(lambda x, y: tf.numpy_function(\n",
        "                            load_image_and_mask,\n",
        "                            [x, y],\n",
        "                            [tf.float32, tf.uint8]),  # Specify output types here\n",
        "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.map(lambda x, y: (tf.reshape(x, [64, 64, 3]), tf.reshape(y, [64, 64, 1])))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "train_dataset = load_dataset('/content/drive/MyDrive/dacl10k_v2_devphase/dacl10k_v2_devphase/images/train', '/content/drive/MyDrive/dacl10k_v2_devphase/dacl10k_v2_devphase/annotations/train')\n",
        "val_dataset = load_dataset('/content/drive/MyDrive/dacl10k_v2_devphase/dacl10k_v2_devphase/images/validation/', '/content/drive/MyDrive/dacl10k_v2_devphase/dacl10k_v2_devphase/annotations/validation/')\n",
        "test_dataset = load_dataset('/content/drive/MyDrive/dacl10k_v2_devphase/dacl10k_v2_devphase/images/test/', '/content/drive/MyDrive/dacl10k_v2_devphase/dacl10k_v2_devphase/annotations/test/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2Rp7CVx1Ijd",
        "outputId": "12d742db-bd87-4e3f-f52d-83c31a4616c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_9 (Conv2D)           (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " separable_conv2d_6 (Separa  (None, 32, 32, 128)       8896      \n",
            " bleConv2D)                                                      \n",
            "                                                                 \n",
            " separable_conv2d_7 (Separa  (None, 32, 32, 256)       34176     \n",
            " bleConv2D)                                                      \n",
            "                                                                 \n",
            " up_sampling2d_3 (UpSamplin  (None, 64, 64, 256)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 64, 64, 128)       295040    \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 64, 64, 19)        21907     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 361811 (1.38 MB)\n",
            "Trainable params: 361811 (1.38 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "def get_optimized_alexnet_segmentation_model(input_shape=(32, 32, 3), num_classes=19):\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "        # Optional: MaxPooling layer can be adjusted or removed depending on the desired level of downsampling\n",
        "\n",
        "        keras.layers.SeparableConv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        # Optional: MaxPooling layer can be adjusted or removed\n",
        "\n",
        "        keras.layers.SeparableConv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        # Optional: MaxPooling layer can be adjusted or removed\n",
        "\n",
        "        # Upsampling to 64x64\n",
        "        keras.layers.UpSampling2D((2, 2)),  # This layer will upscale the feature map to 64x64\n",
        "        keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "\n",
        "        # Final Convolutional Layer for Segmentation Mask\n",
        "        keras.layers.Conv2D(num_classes, (3, 3), activation='softmax', padding='same')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Instantiate the model\n",
        "optimized_model = get_optimized_alexnet_segmentation_model()\n",
        "\n",
        "# Compile the model\n",
        "optimized_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "optimized_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "IFceW-E03nYK",
        "outputId": "b8ea24ba-aad4-4b03-cdcc-9e16175bcf74"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-47f9d0270c71>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minterval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Train for interval_epochs epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Function to calculate metrics for a dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "# Assuming model, train_dataset, val_dataset, and test_dataset are already defined\n",
        "# Assuming calculate_iou, calculate_miou, calculate_accuracy, calculate_loss are defined functions\n",
        "# Assuming num_classes is the number of classes in your dataset\n",
        "\n",
        "total_epochs = 90\n",
        "interval_epochs = 30\n",
        "metrics_history = []\n",
        "num_classes = 19  # Replace with your actual number of classes\n",
        "\n",
        "for interval in range(0, total_epochs, interval_epochs):\n",
        "    # Train for interval_epochs epochs\n",
        "    history = model.fit(train_dataset, epochs=interval_epochs, validation_data=val_dataset)\n",
        "\n",
        "    # Function to calculate metrics for a dataset\n",
        "    def evaluate_dataset(dataset, dataset_name):\n",
        "        iou_sum = np.zeros(num_classes)\n",
        "        miou_sum = np.zeros(num_classes)\n",
        "        accuracy_sum, loss_sum, count = 0, 0, 0\n",
        "        for x_batch, y_batch in dataset:\n",
        "            y_pred = model.predict(x_batch)\n",
        "            for y_true, y_pred_batch in zip(y_batch, y_pred):\n",
        "                iou = calculate_iou(y_true, y_pred_batch)\n",
        "                miou = calculate_miou(y_true, y_pred_batch)\n",
        "                accuracy_sum += calculate_accuracy(y_true, y_pred_batch)\n",
        "                loss_sum += calculate_loss(y_true, y_pred_batch)\n",
        "                iou_sum += iou\n",
        "                miou_sum += miou\n",
        "                count += 1\n",
        "        return iou_sum / count, miou_sum / count, accuracy_sum / count, loss_sum / count\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_iou, val_miou, val_accuracy, val_loss = evaluate_dataset(val_dataset, \"Validation\")\n",
        "    # Evaluate on test set\n",
        "    test_iou, test_miou, test_accuracy, test_loss = evaluate_dataset(test_dataset, \"Test\")\n",
        "\n",
        "    # Save the metrics\n",
        "    metrics_history.append({\n",
        "        'Epoch': interval + interval_epochs,\n",
        "        'Validation IoU': val_iou,\n",
        "        'Validation mIoU': val_miou,\n",
        "        'Validation Accuracy': val_accuracy,\n",
        "        'Validation Loss': val_loss,\n",
        "        'Test IoU': test_iou,\n",
        "        'Test mIoU': test_miou,\n",
        "        'Test Accuracy': test_accuracy,\n",
        "        'Test Loss': test_loss\n",
        "    })\n",
        "\n",
        "    # Save metrics to CSV\n",
        "    metrics_df = pd.DataFrame(metrics_history)\n",
        "    metrics_df.to_csv(f'training_metrics_up_to_epoch_{interval + interval_epochs}.csv', index=False)\n",
        "\n",
        "# Plotting\n",
        "# ... [Plotting code can be added here, modified to handle class-wise IoU and mIoU]\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqxbtHhOPzST"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}